#!/usr/bin/env python3
"""
ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©ëŒ€í•™ êµê³¼ê³¼ì • PDF ìŠ¤í¬ë˜í¼
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from pathlib import Path
import PyPDF2
from typing import List, Dict, Any
import io

class CurriculumScraper:
    def __init__(self):
        self.base_url = "https://software.khu.ac.kr"
        self.list_url = f"{self.base_url}/software/user/bbs/BMSR00048/list.do?menuNo=1700015"
        self.data_dir = Path(__file__).parent.parent / "data"
        self.data_dir.mkdir(exist_ok=True)
        
    def get_pdf_links(self) -> List[Dict[str, str]]:
        """êµê³¼ê³¼ì • PDF ë§í¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        response = requests.get(self.list_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        pdf_links = []
        
        # JavaScript view() í•¨ìˆ˜ì—ì„œ ID ì¶”ì¶œ
        # ì˜ˆ: javascript:view('412977');
        for link in soup.find_all('a', href=re.compile(r"javascript:view")):
            text = link.get_text(strip=True)
            href = link.get('href')
            
            # ID ì¶”ì¶œ
            match = re.search(r"view\('(\d+)'\)", href)
            if match:
                doc_id = match.group(1)
                year_match = re.search(r'(\d{4})í•™ë…„ë„', text)
                
                if year_match:
                    year = year_match.group(1)
                    pdf_links.append({
                        'year': year,
                        'title': text,
                        'doc_id': doc_id,
                        'download_url': f"{self.base_url}/software/user/bbs/BMSR00048/view.do?bbsSeq={doc_id}"
                    })
        
        return sorted(pdf_links, key=lambda x: x['year'], reverse=True)
    
    def download_pdf(self, doc_id: str, year: str) -> bytes:
        """PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ URL êµ¬ì„± (ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        view_url = f"{self.base_url}/software/user/bbs/BMSR00048/view.do?bbsSeq={doc_id}"
        
        # ë¨¼ì € ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼
        session = requests.Session()
        response = session.get(view_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if 'download' in href.lower() or href.endswith('.pdf'):
                pdf_url = self.base_url + href if href.startswith('/') else href
                pdf_response = session.get(pdf_url)
                
                if pdf_response.status_code == 200:
                    # PDF íŒŒì¼ ì €ì¥
                    pdf_path = self.data_dir / f"curriculum_{year}.pdf"
                    with open(pdf_path, 'wb') as f:
                        f.write(pdf_response.content)
                    print(f"âœ… {year}í•™ë…„ë„ PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {pdf_path}")
                    return pdf_response.content
        
        print(f"âš ï¸ {year}í•™ë…„ë„ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        return None
    
    def parse_pdf_text(self, pdf_content: bytes) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()
            return text
        except Exception as e:
            print(f"PDF íŒŒì‹± ì—ëŸ¬: {e}")
            return ""
    
    def parse_courses(self, text: str, year: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê³¼ëª© ì •ë³´ ì¶”ì¶œ"""
        courses = []
        
        # ê³¼ëª© ì •ë³´ íŒ¨í„´ (ì‹¤ì œ PDF êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
        # ì˜ˆ: "SWE001 | í”„ë¡œê·¸ë˜ë°ê¸°ì´ˆ | 3í•™ì  | 1í•™ë…„ 1í•™ê¸°"
        
        lines = text.split('\n')
        for i, line in enumerate(lines):
            line = line.strip()
            
            # ê³¼ëª©ì½”ë“œ íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: SWE001, CSE101 ë“±)
            if re.match(r'^[A-Z]{3}\d{3,4}', line):
                parts = line.split('|')
                
                if len(parts) >= 3:
                    course = {
                        'year': year,
                        'code': parts[0].strip(),
                        'name': parts[1].strip() if len(parts) > 1 else '',
                        'credits': self._extract_credits(parts[2]) if len(parts) > 2 else 3,
                        'semester': self._extract_semester(parts[3]) if len(parts) > 3 else '',
                        'prerequisites': []
                    }
                    courses.append(course)
        
        return courses
    
    def _extract_credits(self, text: str) -> int:
        """í•™ì  ì¶”ì¶œ"""
        match = re.search(r'(\d+)\s*í•™ì ', text)
        return int(match.group(1)) if match else 3
    
    def _extract_semester(self, text: str) -> str:
        """í•™ê¸° ì •ë³´ ì¶”ì¶œ"""
        if '1í•™ê¸°' in text:
            return '1í•™ê¸°'
        elif '2í•™ê¸°' in text:
            return '2í•™ê¸°'
        return ''
    
    def scrape_all(self) -> Dict[str, Any]:
        """ëª¨ë“  êµê³¼ê³¼ì • ìŠ¤í¬ë˜í•‘"""
        print("ğŸ” êµê³¼ê³¼ì • PDF ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        pdf_links = self.get_pdf_links()
        
        all_courses = {}
        
        for link in pdf_links[:3]:  # ìµœê·¼ 3ê°œë…„ë„ë§Œ
            year = link['year']
            doc_id = link['doc_id']
            
            print(f"\nğŸ“„ {year}í•™ë…„ë„ êµê³¼ê³¼ì • ë‹¤ìš´ë¡œë“œ ì¤‘...")
            pdf_content = self.download_pdf(doc_id, year)
            
            if pdf_content:
                print(f"ğŸ“– {year}í•™ë…„ë„ PDF íŒŒì‹± ì¤‘...")
                text = self.parse_pdf_text(pdf_content)
                
                print(f"ğŸ” {year}í•™ë…„ë„ ê³¼ëª© ì •ë³´ ì¶”ì¶œ ì¤‘...")
                courses = self.parse_courses(text, year)
                
                all_courses[year] = {
                    'year': year,
                    'total_courses': len(courses),
                    'courses': courses,
                    'raw_text': text[:1000]  # ì²˜ìŒ 1000ìë§Œ ì €ì¥
                }
                
                print(f"âœ… {year}í•™ë…„ë„: {len(courses)}ê°œ ê³¼ëª© ì¶”ì¶œ ì™„ë£Œ")
        
        # JSON ì €ì¥
        output_file = self.data_dir / "curriculum_data.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_courses, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
        return all_courses


if __name__ == "__main__":
    scraper = CurriculumScraper()
    result = scraper.scrape_all()
    
    print(f"\nğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼:")
    for year, data in result.items():
        print(f"  {year}í•™ë…„ë„: {data['total_courses']}ê°œ ê³¼ëª©")