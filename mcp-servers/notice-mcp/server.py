"""
Notice MCP Server - SSL ê²€ì¦ ìš°íšŒ
"""
import asyncio
import json
import sys
import os
from typing import Any, Dict
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urljoin
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
import time
import urllib3

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# DB ì—°ê²°
backend_path = os.getenv("BACKEND_PATH", "/app")
if backend_path not in sys.path:
    sys.path.insert(0, backend_path)

from app.database import SessionLocal
from app import crud


def build_view_url(base_url: str, href: str) -> str:
    """Convert javascript:view('123') to actual view.do URL preserving menuNo."""
    if not href:
        return ""
    href = href.strip()
    if href.startswith("javascript:view("):
        # Extract boardId safely
        import re
        m = re.search(r"view\(['\"]?(\d+)['\"]?\)", href)
        board_id = m.group(1) if m else ""
        parsed = urlparse(base_url)
        # Replace list.do with view.do
        path = parsed.path.replace("list.do", "view.do")
        qs = parse_qs(parsed.query)
        # Keep menuNo if present
        if board_id:
            qs["boardId"] = [board_id]
        query = urlencode({k: v[0] for k, v in qs.items() if v})
        return urlunparse((parsed.scheme, parsed.netloc, path, "", query, ""))
    return href


def crawl_swedu(limit=20):
    """ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼"""
    url = "http://swcon.khu.ac.kr/post/?mode=list&board_page=1"
    try:
        html = requests.get(
            url,
            timeout=30,
            headers={"User-Agent": "Mozilla/5.0"}
        )
        soup = BeautifulSoup(html.text, 'html.parser')
        rows = soup.find_all('tr')[:limit]
        
        posts = []
        for idx, row in enumerate(rows):
            date_cell = row.find('td', class_="mb-hide-mobile")
            date_text = date_cell.get_text(strip=True) if date_cell else ""
            content = row.find('td', class_="text-left")
            if not content:
                continue
            post_item = content.find('a')
            if not post_item or 'title' not in post_item.attrs:
                continue

            posts.append({
                "id": f"swedu_{idx}_{int(time.time())}",
                "source": "swedu",
                "title": post_item.attrs['title'],
                "content": "",
                "url": post_item.attrs.get('href', ''),
                "date": date_text,
                "author": "ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼",
                "views": 0
            })

            if len(posts) >= limit:
                break
        return posts
    except:
        return []


def crawl_standard(dept_name, base_url, source_code, limit=20, keyword=None, max_pages=3):
    """í‘œì¤€ ê²Œì‹œíŒ - í˜ì´ì§€ë„¤ì´ì…˜ + í‚¤ì›Œë“œ í•„í„°ë§ ì§€ì›"""
    all_posts = []
    
    for page in range(1, max_pages + 1):
        try:
            # URLì— pageIndex íŒŒë¼ë¯¸í„° ì¶”ê°€
            if '?' in base_url:
                url = f"{base_url}&pageIndex={page}" if 'pageIndex=' not in base_url else base_url.replace(f"pageIndex={page-1}", f"pageIndex={page}")
            else:
                url = f"{base_url}?pageIndex={page}"
            
            print(f"  ğŸ“„ {dept_name} í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
            
            resp = requests.get(
                url,
                timeout=15,
                verify=False,  # SSL ê²€ì¦ ìš°íšŒ
                headers={
                    "User-Agent": "Mozilla/5.0",
                    "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
                    "Referer": url,
                    "Connection": "close",
                }
            )
            status = resp.status_code
            # ì¸ì½”ë”© ë³´ì •
            if resp.apparent_encoding:
                resp.encoding = resp.apparent_encoding
            soup = BeautifulSoup(resp.content, 'html.parser')

            # ê¸°ë³¸, ëŒ€ì²´ ì…€ë ‰í„° ëª¨ë‘ ì‹œë„
            selectors = [
                "table.board-list tbody tr",
                "tbody tr",
                "#board tbody tr",
                "table tbody tr",
            ]
            rows = []
            for sel in selectors:
                rows = soup.select(sel)
                if rows:
                    break
    
            # ul/li ê²Œì‹œíŒ í˜•íƒœë„ ì‹œë„
            if not rows:
                li_items = soup.select("ul li")
                if li_items:
                    rows = li_items
    
            if not rows:
                print(f"  âš ï¸ í˜ì´ì§€ {page}: í–‰ ì—†ìŒ")
                break
    
            page_posts = []
            for idx, row in enumerate(rows):
                try:
                    # Check cell count to determine structure
                    cells = row.select('td')
                    
                    # ie.khu.ac.kr/ce.khu.ac.kr: 5 cells [ë¶„ë¥˜1] [ë¶„ë¥˜2] [ì œëª©+ë§í¬] [íŒŒì¼] [ë‚ ì§œ]
                    if len(cells) == 5:
                        title_elem = cells[2].select_one("a")
                        date_elem = cells[4]
                    # Standard boards have 3-4 cells
                    else:
                        title_elem = (
                            row.select_one("td.title a")
                            or row.select_one("td.subj a")
                            or row.select_one("td.subject a")
                            or row.select_one("a")
                        )
                        date_elem = (
                            row.select_one("td.date")
                            or row.select_one("td.regdate")
                            or (row.find_all("td")[-1] if row.find_all("td") else None)
                        )
    
                    if not title_elem or not date_elem:
                        continue
    
                    date_text = date_elem.get_text(strip=True)
                    href = title_elem.get('href', '')
                    href = build_view_url(url, href)
                    if href and not href.startswith('http'):
                        href = urljoin(url, href)
    
                    title_text = title_elem.get_text(strip=True)
                    if not title_text:
                        continue
                    
                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if keyword and keyword.lower() not in title_text.lower():
                        continue
    
                    page_posts.append({
                        "id": f"{source_code}_{page}_{idx}_{int(time.time())}",
                        "source": source_code,
                        "title": title_text,
                        "content": "",
                        "url": href,
                        "date": date_text,
                        "author": dept_name,
                        "views": 0
                    })
    
                except Exception as e:
                    print(f"  âš ï¸ í–‰ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            all_posts.extend(page_posts)
            print(f"  âœ… í˜ì´ì§€ {page}: {len(page_posts)}ê°œ ìˆ˜ì§‘ (í‚¤ì›Œë“œ: {keyword or 'ì „ì²´'})")
            
            # limit ë„ë‹¬ ì‹œ ì¤‘ë‹¨
            if len(all_posts) >= limit:
                break
        
        except Exception as e:
            print(f"  âŒ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            break
    
    return all_posts[:limit]

def crawl_department(dept_query, limit=20, keyword=None):
    """
    DB ê¸°ë°˜ í•™ê³¼ë³„ í¬ë¡¤ë§
    
    Args:
        dept_query: í•™ê³¼ëª… ë˜ëŠ” í•™ê³¼ì½”ë“œ (ì˜ˆ: "ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼", "swedu")
        limit: í¬ë¡¤ë§ ê±´ìˆ˜
        keyword: í•„í„°ë§ í‚¤ì›Œë“œ
    
    Returns:
        ê³µì§€ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # DB ì—°ê²°
        db = SessionLocal()
        
        # í•™ê³¼ ê²€ìƒ‰: ì •í™•í•œ ì´ë¦„ ë˜ëŠ” ì½”ë“œë¡œ ë§¤ì¹­
        dept = db.query(crud.models.Department).filter(
            (crud.models.Department.name == dept_query) |
            (crud.models.Department.code == dept_query)
        ).first()
        
        if not dept:
            print(f"âŒ í•™ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {dept_query}")
            db.close()
            return []
        
        # ê³µì§€ì‚¬í•­ URLì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if not dept.notice_url:
            print(f"âš ï¸ {dept.name}({dept.code})ì€ ì•„ì§ í¬ë¡¤ë§ ì„¤ì •ì´ ë˜ì–´ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            db.close()
            return []
        
        print(f"ğŸ“š {dept.name} ({dept.code}) í¬ë¡¤ë§ ì¤‘...")
        
        # í•™ê³¼ë³„ í¬ë¡¤ë§ ë°©ì‹ ê²°ì •
        if dept.notice_type == "custom":
            # swcon.khu.ac.kr ê°™ì€ ì»¤ìŠ¤í…€ ê²Œì‹œíŒ
            posts = crawl_swedu(limit)
        else:
            # í‘œì¤€ ê²Œì‹œíŒ í˜•ì‹
            posts = crawl_standard(dept.name, dept.notice_url, dept.code, limit, keyword)
        
        db.close()
        return posts
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def _readline():
    line = sys.stdin.readline()
    if not line:
        return None
    try:
        return json.loads(line.strip())
    except:
        return None

def _send(obj: dict):
    sys.stdout.write(json.dumps(obj, ensure_ascii=False) + "\n")
    sys.stdout.flush()

def _result(id_: int, data: Any, is_error: bool = False):
    content = [{"type": "text", "text": json.dumps(data, ensure_ascii=False, indent=2)}]
    _send({"jsonrpc": "2.0", "id": id_, "result": {"content": content, "isError": is_error}})


async def tool_get_latest_notices(args: Dict) -> Dict:
    db = SessionLocal()
    try:
        # department: í•™ê³¼ëª… ë˜ëŠ” í•™ê³¼ì½”ë“œ
        department = args.get("department", "ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼")
        
        # Department í…Œì´ë¸”ì—ì„œ í•™ê³¼ ê²€ìƒ‰
        dept = db.query(crud.models.Department).filter(
            (crud.models.Department.name == department) |
            (crud.models.Department.code == department)
        ).first()
        
        if not dept:
            return {"error": f"í•™ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {department}"}
        
        # department.codeë¥¼ sourceë¡œ ì‚¬ìš©
        notices = crud.get_latest_notices(db, dept.code, args.get("limit", 5))
        return {"notices": [{"title": n.title, "url": n.url, "date": n.date, "author": n.author, "source": n.source, "views": n.views or 0} for n in notices]}
    finally:
        db.close()


async def tool_search_notices(args: Dict) -> Dict:
    db = SessionLocal()
    try:
        notices = crud.search_notices(db, args.get("query", ""), args.get("limit", 5))
        return {"notices": [{"title": n.title, "url": n.url, "date": n.date, "author": n.author, "source": n.source, "views": n.views or 0} for n in notices]}
    finally:
        db.close()


async def tool_crawl_fresh_notices(args: Dict) -> Dict:
    department = args.get("department", "ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼")
    keyword = args.get("keyword")  # í‚¤ì›Œë“œ í•„í„°ë§
    posts = crawl_department(department, args.get("limit", 20), keyword)
    
    # í¬ë¡¤ë§ ì‹¤íŒ¨ë„ ì •ìƒ ì²˜ë¦¬ (ì‹ ê·œ ê³µì§€ ì—†ìŒìœ¼ë¡œ ê°„ì£¼)
    if not posts:
        msg = f"{department} ì‹ ê·œ ê³µì§€ ì—†ìŒ" + (f" (í‚¤ì›Œë“œ: {keyword})" if keyword else "")
        return {"success": True, "department": department, "crawled": 0, "new_count": 0, "message": msg}
    
    db = SessionLocal()
    try:
        new_count = sum(1 for post in posts if crud.create_notice_from_mcp(db, post))
        result = {"success": True, "department": department, "crawled": len(posts), "new_count": new_count}
        if keyword:
            result["keyword"] = keyword
        return result
    finally:
        db.close()


async def main():
    tools = {"get_latest_notices": tool_get_latest_notices, "search_notices": tool_search_notices, "crawl_fresh_notices": tool_crawl_fresh_notices}
    
    while True:
        msg = _readline()
        if msg is None:
            break
        
        if msg.get("method") == "initialize":
            _send({"jsonrpc": "2.0", "id": msg.get("id"), "result": {"protocolVersion": "2024-11-05", "capabilities": {"tools": {}}, "serverInfo": {"name": "notice-mcp", "version": "2.0.0"}}})
        elif msg.get("method") == "tools/list":
            _send({"jsonrpc": "2.0", "id": msg.get("id"), "result": {"tools": [{"name": "get_latest_notices", "description": "í•™ê³¼ë³„ ìµœì‹  ê³µì§€ (í•™ê³¼ëª… ë˜ëŠ” ì½”ë“œë¡œ ê²€ìƒ‰: ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©í•™ê³¼, swedu ë“±)", "inputSchema": {"type": "object", "properties": {"department": {"type": "string"}, "limit": {"type": "integer"}}}}, {"name": "search_notices", "description": "ê³µì§€ì‚¬í•­ ê²€ìƒ‰", "inputSchema": {"type": "object", "properties": {"query": {"type": "string"}, "limit": {"type": "integer"}}, "required": ["query"]}}, {"name": "crawl_fresh_notices", "description": "í•™ê³¼ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ (í‚¤ì›Œë“œ í•„í„°ë§ ì§€ì›)", "inputSchema": {"type": "object", "properties": {"department": {"type": "string", "description": "í•™ê³¼ëª… ë˜ëŠ” ì½”ë“œ (ì˜ˆ: ì‚°ì—…ê²½ì˜ê³µí•™ê³¼, ime)"}, "limit": {"type": "integer"}, "keyword": {"type": "string", "description": "í•„í„°ë§ í‚¤ì›Œë“œ (ì˜ˆ: ì¥í•™ê¸ˆ, ìˆ˜ê°•ì‹ ì²­)"}}}}]}})
        elif msg.get("method") == "tools/call":
            params = msg.get("params", {})
            try:
                result = await tools[params.get("name")](params.get("arguments", {}))
                _result(msg.get("id"), result)
            except Exception as e:
                _result(msg.get("id"), {"error": str(e)}, is_error=True)
        elif "id" in msg:
            _result(msg["id"], {"status": "noop"})
        else:
            continue

if __name__ == "__main__":
    asyncio.run(main())
