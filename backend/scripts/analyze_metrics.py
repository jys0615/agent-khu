#!/usr/bin/env python3
"""
Agent KHU í‰ê°€ ì§€í‘œ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
Elasticsearch ë¡œê·¸ ë˜ëŠ” ë¡œì»¬ JSON ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì§€í‘œ ìƒì„±
"""
import json
import sys
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import statistics
from collections import defaultdict

try:
    from elasticsearch import Elasticsearch
except ImportError:
    print("âš ï¸ elasticsearch ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜: pip install elasticsearch")
    Elasticsearch = None


class MetricsAnalyzer:
    def __init__(self, es_url: Optional[str] = None, raw_results: Optional[List[Dict]] = None):
        self.es_url = es_url or "http://localhost:9200"
        self.es = None
        self.raw_results = raw_results or []
        self.index_name = "agent-khu-interactions"
        self._connect_es()
    
    def _connect_es(self):
        """Elasticsearch ì—°ê²°"""
        if not Elasticsearch:
            print("âš ï¸ Elasticsearch ì‚¬ìš© ë¶ˆê°€ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜)")
            return
        
        try:
            self.es = Elasticsearch([self.es_url])
            if self.es.ping():
                print(f"âœ… Elasticsearch ì—°ê²°: {self.es_url}")
            else:
                self.es = None
                print(f"âš ï¸ Elasticsearch í•‘ ì‹¤íŒ¨")
        except Exception as e:
            print(f"âš ï¸ Elasticsearch ì—°ê²° ì‹¤íŒ¨: {e}")
            self.es = None
    
    def fetch_logs_from_es(self, limit: int = 10000) -> List[Dict]:
        """Elasticsearchì—ì„œ ìµœê·¼ ë¡œê·¸ ì¡°íšŒ"""
        if not self.es:
            print("âš ï¸ Elasticsearch ë¯¸ì—°ê²°: ë¡œì»¬ ë°ì´í„°ë§Œ ì‚¬ìš©")
            return []
        
        try:
            query = {
                "query": {"match_all": {}},
                "sort": [{"timestamp": {"order": "desc"}}],
                "size": limit
            }
            result = self.es.search(index=self.index_name, body=query)
            
            docs = [hit["_source"] for hit in result["hits"]["hits"]]
            print(f"âœ… ESì—ì„œ {len(docs)}ê°œ ë¡œê·¸ ì¡°íšŒ")
            return docs
        
        except Exception as e:
            print(f"âš ï¸ ES ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze(self) -> Dict[str, Any]:
        """ì „ì²´ ë¶„ì„ ìˆ˜í–‰"""
        # ë¡œê·¸ ë°ì´í„° í™•ë³´
        if self.es:
            logs = self.fetch_logs_from_es()
        else:
            logs = self._transform_raw_results()
        
        if not logs:
            print("âŒ ë¶„ì„í•  ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        print(f"\nğŸ“Š {len(logs)}ê°œ ë¡œê·¸ë¡œ ë¶„ì„ ì‹œì‘")
        
        metrics = {
            "metadata": {
                "total_samples": len(logs),
                "timestamp": datetime.utcnow().isoformat(),
                "source": "elasticsearch" if self.es else "local"
            },
            "overall": self._analyze_overall(logs),
            "by_domain": self._analyze_by_domain(logs),
            "latency": self._analyze_latency(logs),
            "tools": self._analyze_tools(logs),
            "routing": self._analyze_routing(logs)
        }
        
        return metrics
    
    def _transform_raw_results(self) -> List[Dict]:
        """ë¡œì»¬ í‰ê°€ ê²°ê³¼ë¥¼ ë¡œê·¸ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        logs = []
        for result in self.raw_results:
            if result.get("status") == "success":
                logs.append({
                    "question": result.get("question", ""),
                    "success": True,
                    "latency_ms": result.get("latency_ms", 0),
                    "question_type": self._infer_type(result.get("domain", "")),
                    "routing_decision": "llm",  # ê¸°ë³¸ê°’
                    "mcp_tools_used": result.get("tools_used", []),
                    "timestamp": result.get("timestamp", datetime.utcnow().isoformat()),
                    "domain": result.get("domain", "unknown")
                })
            else:
                logs.append({
                    "question": result.get("question", ""),
                    "success": False,
                    "latency_ms": result.get("latency_ms", 0),
                    "question_type": self._infer_type(result.get("domain", "")),
                    "error_message": result.get("error", ""),
                    "timestamp": result.get("timestamp", datetime.utcnow().isoformat()),
                    "domain": result.get("domain", "unknown")
                })
        return logs
    
    def _infer_type(self, domain: str) -> str:
        """ë„ë©”ì¸ì„ question_typeìœ¼ë¡œ ë³€í™˜"""
        if domain in ["classroom", "curriculum", "notice", "library", "meal", "shuttle"]:
            return "simple"
        return "complex"
    
    def _analyze_overall(self, logs: List[Dict]) -> Dict:
        """ì „ì²´ ì„±ê³µë¥ , ì§€ì—° ë“±"""
        success_count = sum(1 for log in logs if log.get("success", False))
        total = len(logs)
        success_rate = (success_count / total * 100) if total > 0 else 0
        
        latencies = [log.get("latency_ms", 0) for log in logs if log.get("success", False)]
        
        return {
            "success_rate": round(success_rate, 2),
            "success_count": success_count,
            "total": total,
            "failed_count": total - success_count,
            "failed_rate": round(100 - success_rate, 2)
        }
    
    def _analyze_by_domain(self, logs: List[Dict]) -> Dict:
        """ë„ë©”ì¸ë³„ ë¶„ì„"""
        by_domain = defaultdict(list)
        
        for log in logs:
            domain = log.get("domain", "unknown")
            by_domain[domain].append(log)
        
        result = {}
        for domain, domain_logs in by_domain.items():
            success = sum(1 for log in domain_logs if log.get("success", False))
            total = len(domain_logs)
            latencies = [log.get("latency_ms", 0) for log in domain_logs if log.get("success", False)]
            
            result[domain] = {
                "total": total,
                "success": success,
                "success_rate": round(success / total * 100, 2) if total > 0 else 0,
                "avg_latency_ms": round(statistics.mean(latencies), 2) if latencies else 0,
                "p50_latency_ms": round(statistics.median(latencies), 2) if latencies else 0,
                "p95_latency_ms": round(self._percentile(latencies, 95), 2) if latencies else 0
            }
        
        return result
    
    def _analyze_latency(self, logs: List[Dict]) -> Dict:
        """ì§€ì—° ë¶„ì„"""
        latencies = [log.get("latency_ms", 0) for log in logs if log.get("success", False)]
        
        if not latencies:
            return {"error": "No successful requests"}
        
        sorted_latencies = sorted(latencies)
        
        return {
            "min_ms": min(latencies),
            "max_ms": max(latencies),
            "avg_ms": round(statistics.mean(latencies), 2),
            "median_ms": round(statistics.median(latencies), 2),
            "p50_ms": round(self._percentile(latencies, 50), 2),
            "p95_ms": round(self._percentile(latencies, 95), 2),
            "p99_ms": round(self._percentile(latencies, 99), 2),
            "stdev_ms": round(statistics.stdev(latencies), 2) if len(latencies) > 1 else 0
        }
    
    def _analyze_tools(self, logs: List[Dict]) -> Dict:
        """Tool ì‚¬ìš© ë¶„ì„"""
        tool_counts = []
        all_tools = defaultdict(int)
        
        for log in logs:
            if log.get("success", False):
                tools = log.get("mcp_tools_used", [])
                if isinstance(tools, list):
                    tool_counts.append(len(tools))
                    for tool in tools:
                        all_tools[tool] += 1
        
        return {
            "avg_tools_per_request": round(statistics.mean(tool_counts), 2) if tool_counts else 0,
            "max_tools": max(tool_counts) if tool_counts else 0,
            "zero_tool_requests": sum(1 for c in tool_counts if c == 0),
            "zero_tool_rate": round(sum(1 for c in tool_counts if c == 0) / len(tool_counts) * 100, 2) if tool_counts else 0,
            "most_used_tools": dict(sorted(all_tools.items(), key=lambda x: x[1], reverse=True)[:5])
        }
    
    def _analyze_routing(self, logs: List[Dict]) -> Dict:
        """ë¼ìš°íŒ… ê²°ì • ë¶„ì„"""
        routing_counts = defaultdict(int)
        
        for log in logs:
            decision = log.get("routing_decision", "unknown")
            routing_counts[decision] += 1
        
        total = sum(routing_counts.values())
        
        return {
            "distribution": {k: round(v / total * 100, 2) for k, v in routing_counts.items()} if total > 0 else {},
            "counts": dict(routing_counts)
        }
    
    @staticmethod
    def _percentile(values: List[float], p: int) -> float:
        """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
        if not values:
            return 0
        sorted_values = sorted(values)
        index = int(len(sorted_values) * p / 100)
        return sorted_values[min(index, len(sorted_values) - 1)]
    
    def print_report(self, metrics: Dict):
        """ë³´ê³ ì„œ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“ Agent KHU í‰ê°€ ê²°ê³¼ ë³´ê³ ì„œ")
        print("=" * 80)
        
        meta = metrics.get("metadata", {})
        print(f"\nğŸ“Š ê¸°ë³¸ ì •ë³´")
        print(f"   ìƒ˜í”Œ ìˆ˜: {meta.get('total_samples', 'N/A')}")
        print(f"   ë°ì´í„° ì†ŒìŠ¤: {meta.get('source', 'unknown')}")
        print(f"   ìƒì„± ì‹œê°„: {meta.get('timestamp', 'N/A')[:19]}")
        
        # ì „ì²´ ì„±ê³µë¥ 
        overall = metrics.get("overall", {})
        print(f"\nâœ… ì „ì²´ ì„±ê³µ ì§€í‘œ")
        print(f"   ì„±ê³µë¥ : {overall.get('success_rate', 0)}%")
        print(f"   ì„±ê³µ: {overall.get('success_count', 0)} / {overall.get('total', 0)}")
        print(f"   ì‹¤íŒ¨: {overall.get('failed_count', 0)}")
        
        # ì§€ì—°
        latency = metrics.get("latency", {})
        print(f"\nâ±ï¸  ì‘ë‹µ ì§€ì—° (ì„±ê³µí•œ ìš”ì²­)")
        print(f"   ìµœì†Œ: {latency.get('min_ms', 0):>6.0f}ms")
        print(f"   P50 : {latency.get('p50_ms', 0):>6.2f}ms")
        print(f"   P95 : {latency.get('p95_ms', 0):>6.2f}ms")
        print(f"   P99 : {latency.get('p99_ms', 0):>6.2f}ms")
        print(f"   ìµœëŒ€: {latency.get('max_ms', 0):>6.0f}ms")
        print(f"   í‰ê· : {latency.get('avg_ms', 0):>6.2f}ms (Ïƒ={latency.get('stdev_ms', 0):.2f})")
        
        # Tool ì‚¬ìš©
        tools = metrics.get("tools", {})
        print(f"\nğŸ”§ Tool ì‚¬ìš© í˜„í™©")
        print(f"   í‰ê·  Tool/ìš”ì²­: {tools.get('avg_tools_per_request', 0):.2f}")
        print(f"   0íšŒ ë°˜ë³µ: {tools.get('zero_tool_requests', 0)} ({tools.get('zero_tool_rate', 0)}%)")
        print(f"   ìµœëŒ€ Tool ìˆ˜: {tools.get('max_tools', 0)}")
        
        most_used = tools.get("most_used_tools", {})
        if most_used:
            print(f"   ìƒìœ„ ë„êµ¬:")
            for tool, count in list(most_used.items())[:3]:
                print(f"      - {tool}: {count}íšŒ")
        
        # ë¼ìš°íŒ…
        routing = metrics.get("routing", {})
        print(f"\nğŸ›£ï¸  ë¼ìš°íŒ… ê²°ì • ë¶„í¬")
        dist = routing.get("distribution", {})
        for decision, percentage in dist.items():
            print(f"   {decision}: {percentage}%")
        
        # ë„ë©”ì¸ë³„ ì„±ê³µë¥ 
        by_domain = metrics.get("by_domain", {})
        if by_domain:
            print(f"\nğŸ“ˆ ë„ë©”ì¸ë³„ ì„±ëŠ¥")
            print(f"   {'ë„ë©”ì¸':<15} {'ì„±ê³µë¥ ':<10} {'P50':<10} {'P95':<10} {'ìƒ˜í”Œ':<8}")
            print(f"   {'-'*53}")
            for domain, stats in sorted(by_domain.items()):
                success_rate = stats.get("success_rate", 0)
                p50 = stats.get("p50_latency_ms", 0)
                p95 = stats.get("p95_latency_ms", 0)
                total = stats.get("total", 0)
                print(f"   {domain:<15} {success_rate:>6.1f}%   {p50:>7.1f}ms  {p95:>7.1f}ms  {total:>5d}")
        
        print("\n" + "=" * 80)
    
    def save_report(self, metrics: Dict, output_file: Path):
        """ë³´ê³ ì„œë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ JSON ë³´ê³ ì„œ ì €ì¥: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="Agent KHU í‰ê°€ ë¶„ì„")
    parser.add_argument("--es_url", type=str, default="http://localhost:9200", help="Elasticsearch URL")
    parser.add_argument("--input", type=str, help="ë¡œì»¬ í‰ê°€ ê²°ê³¼ JSON íŒŒì¼")
    parser.add_argument("--output", type=str, default=None, help="ë¶„ì„ ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    args = parser.parse_args()
    
    # ë¡œì»¬ ê²°ê³¼ ë¡œë“œ (ìˆìœ¼ë©´)
    raw_results = []
    if args.input:
        input_path = Path(args.input)
        if input_path.exists():
            with open(input_path) as f:
                raw_results = json.load(f)
            print(f"âœ… ë¡œì»¬ ê²°ê³¼ ë¡œë“œ: {input_path} ({len(raw_results)}ê°œ)")
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = MetricsAnalyzer(es_url=args.es_url, raw_results=raw_results)
    metrics = analyzer.analyze()
    
    if not metrics:
        print("âŒ ë¶„ì„ ì‹¤íŒ¨")
        sys.exit(1)
    
    # ë³´ê³ ì„œ ì¶œë ¥
    analyzer.print_report(metrics)
    
    # ì €ì¥
    output_file = Path(args.output) if args.output else Path(__file__).parent / "evaluation_metrics.json"
    analyzer.save_report(metrics, output_file)
    
    print(f"\nâœ¨ ë¶„ì„ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
