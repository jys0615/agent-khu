"""
Bllossom-8B íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
"""
import os
import json
import torch
from datetime import datetime
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model
from datasets import Dataset
import subprocess


# ê²½ë¡œ ì„¤ì • (backend/scripts ê¸°ì¤€)
SCRIPT_DIR = Path(__file__).parent  # backend/scripts
BACKEND_DIR = SCRIPT_DIR.parent      # backend
PROJECT_ROOT = BACKEND_DIR.parent    # agent-khu

MODELS_DIR = PROJECT_ROOT / "models"
CHECKPOINTS_DIR = MODELS_DIR / "checkpoints"
FINETUNED_DIR = MODELS_DIR / "finetuned"
TRAINING_DATA_FILE = SCRIPT_DIR / "training_data.jsonl"
LOGS_DIR = PROJECT_ROOT / "logs"

# ë””ë ‰í† ë¦¬ ìƒì„±
MODELS_DIR.mkdir(exist_ok=True)
CHECKPOINTS_DIR.mkdir(exist_ok=True)
FINETUNED_DIR.mkdir(exist_ok=True)
LOGS_DIR.mkdir(exist_ok=True)


def extract_training_data():
    """Elasticsearchì—ì„œ í•™ìŠµ ë°ì´í„° ì¶”ì¶œ"""
    print("ğŸ“Š í•™ìŠµ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    
    # extract_training_data.py ì‹¤í–‰
    result = subprocess.run(
        ["python3", str(SCRIPT_DIR / "extract_training_data.py")],
        capture_output=True,
        text=True,
        cwd=str(SCRIPT_DIR)
    )
    
    if result.returncode != 0:
        print(f"âŒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {result.stderr}")
        return False
    
    print(result.stdout)
    return True


def load_training_data():
    """JSONL íŒŒì¼ì—ì„œ í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    if not TRAINING_DATA_FILE.exists():
        print(f"âŒ í•™ìŠµ ë°ì´í„° íŒŒì¼ ì—†ìŒ: {TRAINING_DATA_FILE}")
        return None
    
    data = []
    with open(TRAINING_DATA_FILE, "r", encoding="utf-8") as f:
        for line in f:
            data.append(json.loads(line))
    
    print(f"âœ… {len(data)}ê°œ í•™ìŠµ ë°ì´í„° ë¡œë“œ")
    return data


def format_prompt(example):
    """Bllossom í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
    return f"""### ì§ˆë¬¸: {example['input']}

### ë‹µë³€: {example['output']}"""


def prepare_dataset(data, tokenizer):
    """ë°ì´í„°ì…‹ ì¤€ë¹„"""
    formatted_data = []
    for example in data:
        formatted_data.append({
            "text": format_prompt(example)
        })
    
    dataset = Dataset.from_list(formatted_data)
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=512,
            padding="max_length"
        )
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    return tokenized_dataset


def finetune():
    """Bllossom-8B íŒŒì¸íŠœë‹ ì‹¤í–‰"""
    print("\nğŸš€ Bllossom-8B íŒŒì¸íŠœë‹ ì‹œì‘")
    print(f"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # 1. í•™ìŠµ ë°ì´í„° ì¶”ì¶œ
    if not extract_training_data():
        return False
    
    # 2. ë°ì´í„° ë¡œë“œ
    data = load_training_data()
    if not data or len(data) < 10:
        print("âŒ í•™ìŠµ ë°ì´í„° ë¶€ì¡± (ìµœì†Œ 10ê°œ í•„ìš”)")
        return False
    
    # 3. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\nğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
    # Qwen 2.5 3B Instruct (HF weights; GGUFëŠ” í•™ìŠµ ë¶ˆê°€)
    model_name = "Qwen/Qwen2.5-3B-Instruct"

    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    tokenizer.pad_token = tokenizer.eos_token

    # CPU í™˜ê²½ì—ì„œ bitsandbytes 4bitê°€ ë™ì‘í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê¸°ë³¸ ë¡œë”©ìœ¼ë¡œ ì „í™˜
    # (ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ í¬ë©´ ë” ì‘ì€ ëª¨ë¸ë¡œ êµì²´ í•„ìš”)
    device_map = "auto" if torch.cuda.is_available() else {"": "cpu"}
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map=device_map,
        torch_dtype=dtype,
        trust_remote_code=True
    )
    
    # 4. LoRA ì„¤ì •
    print("ğŸ”§ LoRA ì„¤ì • ì¤‘...")
    
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 5. ë°ì´í„°ì…‹ ì¤€ë¹„
    print("\nğŸ“š ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
    dataset = prepare_dataset(data, tokenizer)
    
    # 6. í•™ìŠµ ì„¤ì •
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = CHECKPOINTS_DIR / f"bllossom-khu-{timestamp}"
    
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=3,
        per_device_train_batch_size=1 if not torch.cuda.is_available() else 4,
        gradient_accumulation_steps=1 if not torch.cuda.is_available() else 4,
        learning_rate=2e-4,
        fp16=torch.cuda.is_available(),
        bf16=False,
        logging_steps=10,
        save_steps=100,
        save_total_limit=3,
        warmup_steps=10,
        report_to="tensorboard",
        logging_dir=str(output_dir / "logs")
    )
    
    # 7. Trainer ì„¤ì •
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
    )
    
    # 8. í•™ìŠµ ì‹œì‘
    print("\nğŸ“ í•™ìŠµ ì‹œì‘...\n")
    trainer.train()
    
    # 9. ëª¨ë¸ ì €ì¥
    final_model_dir = FINETUNED_DIR / f"bllossom-khu-{timestamp}"
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {final_model_dir}")
    
    trainer.model.save_pretrained(str(final_model_dir))
    tokenizer.save_pretrained(str(final_model_dir))
    
    # 10. ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "model_name": model_name,
        "timestamp": timestamp,
        "num_samples": len(data),
        "epochs": 3,
        "output_dir": str(final_model_dir)
    }
    
    with open(final_model_dir / "metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nâœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print(f"ğŸ“ ëª¨ë¸ ìœ„ì¹˜: {final_model_dir}")
    print(f"â° ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return True


if __name__ == "__main__":
    try:
        success = finetune()
        exit(0 if success else 1)
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        exit(1)