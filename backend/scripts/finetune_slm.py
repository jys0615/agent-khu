"""
Bllossom-8B νμΈνλ‹ μ¤ν¬λ¦½νΈ
"""
import os
import json
import torch
from datetime import datetime
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import subprocess


# κ²½λ΅ μ„¤μ • (backend/scripts κΈ°μ¤€)
SCRIPT_DIR = Path(__file__).parent  # backend/scripts
BACKEND_DIR = SCRIPT_DIR.parent      # backend
PROJECT_ROOT = BACKEND_DIR.parent    # agent-khu

MODELS_DIR = PROJECT_ROOT / "models"
CHECKPOINTS_DIR = MODELS_DIR / "checkpoints"
FINETUNED_DIR = MODELS_DIR / "finetuned"
TRAINING_DATA_FILE = SCRIPT_DIR / "training_data.jsonl"
LOGS_DIR = PROJECT_ROOT / "logs"

# λ””λ ‰ν† λ¦¬ μƒμ„±
MODELS_DIR.mkdir(exist_ok=True)
CHECKPOINTS_DIR.mkdir(exist_ok=True)
FINETUNED_DIR.mkdir(exist_ok=True)
LOGS_DIR.mkdir(exist_ok=True)


def extract_training_data():
    """Elasticsearchμ—μ„ ν•™μµ λ°μ΄ν„° μ¶”μ¶"""
    print("π“ ν•™μµ λ°μ΄ν„° μ¶”μ¶ μ¤‘...")
    
    # extract_training_data.py μ‹¤ν–‰
    result = subprocess.run(
        ["python3", str(SCRIPT_DIR / "extract_training_data.py")],
        capture_output=True,
        text=True,
        cwd=str(SCRIPT_DIR)
    )
    
    if result.returncode != 0:
        print(f"β λ°μ΄ν„° μ¶”μ¶ μ‹¤ν¨: {result.stderr}")
        return False
    
    print(result.stdout)
    return True


def load_training_data():
    """JSONL νμΌμ—μ„ ν•™μµ λ°μ΄ν„° λ΅λ“"""
    if not TRAINING_DATA_FILE.exists():
        print(f"β ν•™μµ λ°μ΄ν„° νμΌ μ—†μ: {TRAINING_DATA_FILE}")
        return None
    
    data = []
    with open(TRAINING_DATA_FILE, "r", encoding="utf-8") as f:
        for line in f:
            data.append(json.loads(line))
    
    print(f"β… {len(data)}κ° ν•™μµ λ°μ΄ν„° λ΅λ“")
    return data


def format_prompt(example):
    """Bllossom ν•μ‹μΌλ΅ ν”„λ΅¬ν”„νΈ ν¬λ§·ν…"""
    return f"""### μ§λ¬Έ: {example['input']}

### λ‹µλ³€: {example['output']}"""


def prepare_dataset(data, tokenizer):
    """λ°μ΄ν„°μ…‹ μ¤€λΉ„"""
    formatted_data = []
    for example in data:
        formatted_data.append({
            "text": format_prompt(example)
        })
    
    dataset = Dataset.from_list(formatted_data)
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=512,
            padding="max_length"
        )
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    return tokenized_dataset


def finetune():
    """Bllossom-8B νμΈνλ‹ μ‹¤ν–‰"""
    print("\nπ€ Bllossom-8B νμΈνλ‹ μ‹μ‘")
    print(f"π“ ν”„λ΅μ νΈ λ£¨νΈ: {PROJECT_ROOT}")
    print(f"β° μ‹μ‘ μ‹κ°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # 1. ν•™μµ λ°μ΄ν„° μ¶”μ¶
    if not extract_training_data():
        return False
    
    # 2. λ°μ΄ν„° λ΅λ“
    data = load_training_data()
    if not data or len(data) < 10:
        print("β ν•™μµ λ°μ΄ν„° λ¶€μ΅± (μµμ† 10κ° ν•„μ”)")
        return False
    
    # 3. λ¨λΈ & ν† ν¬λ‚μ΄μ € λ΅λ“
    print("\nπ“¦ λ¨λΈ λ΅λ”© μ¤‘...")
    model_name = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # 4-bit μ–‘μν™”λ΅ λ©”λ¨λ¦¬ μ μ•½
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_4bit=True,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    
    # 4. LoRA μ„¤μ •
    print("π”§ LoRA μ„¤μ • μ¤‘...")
    model = prepare_model_for_kbit_training(model)
    
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 5. λ°μ΄ν„°μ…‹ μ¤€λΉ„
    print("\nπ“ λ°μ΄ν„°μ…‹ μ¤€λΉ„ μ¤‘...")
    dataset = prepare_dataset(data, tokenizer)
    
    # 6. ν•™μµ μ„¤μ •
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = CHECKPOINTS_DIR / f"bllossom-khu-{timestamp}"
    
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        save_steps=100,
        save_total_limit=3,
        warmup_steps=50,
        report_to="tensorboard",
        logging_dir=str(output_dir / "logs")
    )
    
    # 7. Trainer μ„¤μ •
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
    )
    
    # 8. ν•™μµ μ‹μ‘
    print("\nπ“ ν•™μµ μ‹μ‘...\n")
    trainer.train()
    
    # 9. λ¨λΈ μ €μ¥
    final_model_dir = FINETUNED_DIR / f"bllossom-khu-{timestamp}"
    print(f"\nπ’Ύ λ¨λΈ μ €μ¥ μ¤‘: {final_model_dir}")
    
    trainer.model.save_pretrained(str(final_model_dir))
    tokenizer.save_pretrained(str(final_model_dir))
    
    # 10. λ©”νƒ€λ°μ΄ν„° μ €μ¥
    metadata = {
        "model_name": model_name,
        "timestamp": timestamp,
        "num_samples": len(data),
        "epochs": 3,
        "output_dir": str(final_model_dir)
    }
    
    with open(final_model_dir / "metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nβ… νμΈνλ‹ μ™„λ£!")
    print(f"π“ λ¨λΈ μ„μΉ: {final_model_dir}")
    print(f"β° μΆ…λ£ μ‹κ°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return True


if __name__ == "__main__":
    try:
        success = finetune()
        exit(0 if success else 1)
    except Exception as e:
        print(f"\nβ μ—λ¬ λ°μƒ: {e}")
        import traceback
        traceback.print_exc()
        exit(1)